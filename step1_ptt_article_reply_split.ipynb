{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. set file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zoe/Desktop/ptt_crawler/local_script\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gossiping-1-5.json',\n",
       " 'Gossiping-39361-39363_reply_20191228_191753.csv',\n",
       " 'ntusd-negative.txt',\n",
       " 'ptt_id_check.ipynb',\n",
       " '.DS_Store',\n",
       " 'Gossiping-28800-29600.json',\n",
       " 'ptt_excel.xlsx',\n",
       " 'Gossiping-1-5_article_20191228_201931.csv',\n",
       " 'Gossiping-1-5_article_20191228_191753.csv',\n",
       " 'local_script',\n",
       " 'neo4j_community_pagerank_1204-20191211T114233Z-001.zip',\n",
       " 'python -m PttWebCrawler -b PublicServan -i 100 200',\n",
       " 'Gossiping-39361-39363.json',\n",
       " 'test.json',\n",
       " 'Gossiping-37500-39075.json',\n",
       " 'ntusd-positive.txt',\n",
       " 'neo4j_community_pagerank_1204',\n",
       " 'Gossiping-39361-39363_article_20191228_191753.csv',\n",
       " '.ipynb_checkpoints',\n",
       " 'Gossiping-37700-39294_1207.json',\n",
       " 'test2.json',\n",
       " 'Gossiping-1-5_reply_20191228_201931.csv',\n",
       " 'Gossiping-1-5_reply_20191228_191753.csv',\n",
       " 'HatePolitics-2600-4035.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "path = r'/Users/zoe/Desktop/ptt_crawler/'\n",
    "exec_dt = datetime.strftime(datetime.now(), '%Y%m%d_%H%M%S')\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. check json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_json_format(file):\n",
    "    with open(file, 'ab+') as f:\n",
    "        # read as binary is for reverse file search\n",
    "        print('check trivial comma in EOF before revesing')\n",
    "        # * 2 -- end of stream; offset is usually negative\n",
    "        f.seek(-6, 2)\n",
    "  \n",
    "        if f.read(200) == b'}]},]}':\n",
    "            print('revising error endings')\n",
    "            f.seek(-3, 2)\n",
    "            f.truncate()\n",
    "            f.write(b']}')\n",
    "  \n",
    "        print('check trivial comma in EOF after revising')\n",
    "        f.seek(-20, 2)\n",
    "        print(f.read(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. parse json file\n",
    "## notes:\n",
    "#### 1. bug of special character in nickname\n",
    "#### ex: rex520368 (b@N9) https://www.ptt.cc/bbs/Lifeismoney/M.1575016206.A.74B.html\n",
    "#### check method: art_df[art_df.author == 'NoData'] \n",
    "#### 2. remove filter feature: (1) no reply (2) unknow author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptt_json_to_df(ptt_df):\n",
    "    art_df = []\n",
    "    #currently ignore error records\n",
    "    invalid_lst = []\n",
    "    \n",
    "    for x in ptt_df.itertuples():\n",
    "        #some inconsistent page failed!\n",
    "        if x.articles == {'error': 'invalid url'}:\n",
    "            print('invalid record')\n",
    "            print(x.Index)\n",
    "            invalid_lst.append(x)\n",
    "            continue\n",
    "        art = pd.DataFrame([x.articles])\n",
    "        \n",
    "        if art.author[0] is None:\n",
    "            art.author = 'NoData'    \n",
    "        else:\n",
    "            art.author = art.author.replace(r'\\([^)]*\\)', '',regex=True).values[0].replace(' ', '')\n",
    "        \n",
    "        #fixing package typo of columns\n",
    "        art.rename({'message_conut':'message_count'}, axis=1, inplace=True)\n",
    "        reply_aggr = pd.DataFrame(art.message_count.values[0], index=[0])\n",
    "        art.drop(['message_count'], axis=1, inplace=True)\n",
    "        art = pd.concat([art, reply_aggr], axis=1)\n",
    "        art_df.append(art)\n",
    "    art_df = pd.concat(art_df)\n",
    "    art_df.reset_index(drop=True, inplace=True)\n",
    "    return art_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptt_art_parser(f_nm):\n",
    "    ##this parser is for jwlin crawler's result\n",
    "    print('{} parsing start!'.format(f_nm))\n",
    "    ptt_df = pd.read_json(f_nm, encoding='utf8')    \n",
    "    ptt_df_t1 = ptt_json_to_df(ptt_df)\n",
    "    ptt_df_t1.date = pd.to_datetime(ptt_df_t1.date, format='%a %b %d %H:%M:%S %Y', errors='coerce')\n",
    "    print('parsing finished!')\n",
    "    return ptt_df_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. reply parsing\n",
    "## fix this incompleted parser....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptt_reply_parser(art_df):\n",
    "    reply_df = []\n",
    "    pct_20 = int(art_df.shape[0]/20)\n",
    "    print('Total iter is {}'.format(art_df.shape[0]))\n",
    "\n",
    "    for idx, x in enumerate(art_df.itertuples()):\n",
    "        if idx % pct_20 == 0:\n",
    "            print(idx)\n",
    "            print(datetime.now())\n",
    "            gc.collect()\n",
    "\n",
    "        if x.messages == []:\n",
    "            continue\n",
    "        tmp_reply = pd.DataFrame(x.messages)\n",
    "        tmp_reply['ip'] = tmp_reply.push_ipdatetime.str.split(' ', n=1, expand=True)[0]\n",
    "        tmp_ip_dt = tmp_reply.push_ipdatetime.copy()\n",
    "        tmp_dt = np.where(tmp_ip_dt.str.len() < 20, tmp_ip_dt, tmp_ip_dt.str[-11:])\n",
    "\n",
    "        #add ip....\n",
    "        if type(x.date) is not str:\n",
    "            tmp_reply['reply_datetime'] = str(x.date.year) + '/' + tmp_dt\n",
    "        else:\n",
    "            tmp_reply['reply_datetime'] = tmp_dt\n",
    "        ##以發文日期作為推文的估計值，因為push_ipdatetime的ip 跟 datetime放在一起，有些只有datetime 沒有ip，好像跟RE 有關....\n",
    "        ##欄位重新命名，push_userid 改為 source\n",
    "\n",
    "        tmp_reply['author'] = x.author\n",
    "        tmp_reply['article_id'] = x.article_id\n",
    "        tmp_reply['board'] = x.board\n",
    "        tmp_reply['date'] = x.date\n",
    "\n",
    "        reply_df.append(tmp_reply)\n",
    "    reply_df = pd.concat(reply_df)\n",
    "    return reply_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. intergrate whole function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intg_parser(path, file_nm):\n",
    "    file_path = path + file_nm\n",
    "    check_json_format(file_path)\n",
    "    art_df = ptt_art_parser(file_path)\n",
    "    art_df['re_flag'] = np.where(art_df.article_title.str.contains('Re'), 'Y', 'N')\n",
    "    art_df['expl_flag'] = np.where(art_df['count'] >= 100, 'Y', 'N')\n",
    "    #avoid comma parse error in csv\n",
    "    art_df.replace(',', '，', regex=True, inplace=True)\n",
    "\n",
    "    #selected output article\n",
    "    slc_col = ['article_id', 'article_title', 'author', 'board', 'content', 'date',\n",
    "               'ip', 'all', 'boo', 'count', 'neutral', 'push', 're_flag', 'expl_flag']\n",
    "    #output article csv\n",
    "    art_df[slc_col].to_csv(path + file_nm.strip('.json') + '_article_{}.csv'.format(exec_dt))\n",
    "\n",
    "    #parsing reply\n",
    "    reply_df = ptt_reply_parser(art_df)\n",
    "    reply_df.reset_index(drop=True, inplace=True)\n",
    "    reply_df.replace(',', '，', regex=True, inplace=True)\n",
    "    reply_df[reply_df.push_content.str.contains(',')]\n",
    "\n",
    "    #not transfer format to datetime(pd.to_datetime(df, errors='coerce')), because there are some elements without year.\n",
    "    reply_df.reply_datetime.replace('/', '-', regex=True, inplace=True)\n",
    "    #output reply csv\n",
    "    reply_df.to_csv(path + file_nm.strip('.json') + '_reply_{}.csv'.format(exec_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. exec whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gossiping-1-5.json\n",
      "check trivial comma in EOF before revesing\n",
      "check trivial comma in EOF after revising\n",
      "b' \"alligator176\"}]}]}'\n",
      "/Users/zoe/Desktop/ptt_crawler/Gossiping-1-5.json parsing start!\n",
      "parsing finished!\n",
      "Total iter is 100\n",
      "0\n",
      "2019-12-28 20:19:32.244067\n",
      "5\n",
      "2019-12-28 20:19:32.296363\n",
      "10\n",
      "2019-12-28 20:19:32.340842\n",
      "15\n",
      "2019-12-28 20:19:32.387559\n",
      "20\n",
      "2019-12-28 20:19:32.428973\n",
      "25\n",
      "2019-12-28 20:19:32.473862\n",
      "30\n",
      "2019-12-28 20:19:32.517379\n",
      "35\n",
      "2019-12-28 20:19:32.561865\n",
      "40\n",
      "2019-12-28 20:19:32.605348\n",
      "45\n",
      "2019-12-28 20:19:32.643809\n",
      "50\n",
      "2019-12-28 20:19:32.685874\n",
      "55\n",
      "2019-12-28 20:19:32.730852\n",
      "60\n",
      "2019-12-28 20:19:32.778437\n",
      "65\n",
      "2019-12-28 20:19:32.817650\n",
      "70\n",
      "2019-12-28 20:19:32.856731\n",
      "75\n",
      "2019-12-28 20:19:32.896824\n",
      "80\n",
      "2019-12-28 20:19:32.937059\n",
      "85\n",
      "2019-12-28 20:19:32.980677\n",
      "90\n",
      "2019-12-28 20:19:33.024373\n",
      "95\n",
      "2019-12-28 20:19:33.069943\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(path):\n",
    "    if file.endswith('json'):\n",
    "        print(file)\n",
    "        intg_parser(path=path, file_nm=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
